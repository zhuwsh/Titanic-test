{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据读入及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software_installed\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n",
      "D:\\software_installed\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#读取训练数据\n",
    "data = pd.read_csv(\"data/train.csv\")\n",
    "#查看数据情况\n",
    "data.info()\n",
    "\n",
    "#将Sex列数据转换为1或0\n",
    "data['Sex'] = data['Sex'].apply(lambda s : 1 if s == 'male' else 0)\n",
    "#缺失字段填充为0\n",
    "data = data.fillna(0)\n",
    "#选择以下特征用于分类\n",
    "dataset_X = data[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']]\n",
    "dataset_X = dataset_X.as_matrix()\n",
    "\n",
    "#两种分类分别是幸存和死亡，‘Survived’字段是其中一种分类的标签\n",
    "#新增加'Deceased'字段表示第二种分类的标签，取值为'Survived'取非\n",
    "data['Deceased'] = data['Survived'].apply(lambda s : int(not s))\n",
    "dataset_Y = data[['Deceased', 'Survived']]\n",
    "dataset_Y = dataset_Y.as_matrix()\n",
    "#在训练数据中选择20%数据用来进行测试\n",
    "X_train, X_val, y_train, y_val = train_test_split(dataset_X, dataset_Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.构建计算图，采用逻辑回归进行构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software_installed\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "#声明输入数据占位符\n",
    "#shape参数的第一个元素为None,表示可以同时放入任意条记录，每条记录都有6个特征\n",
    "X = tf.placeholder(tf.float32, shape=[None, 6])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "#声明参数变量权重W和bias\n",
    "W = tf.Variable(tf.random_normal([6, 2]), name='weights')\n",
    "bias = tf.Variable(tf.zeros([2]), name='bias')\n",
    "\n",
    "#构造前向传播计算图\n",
    "y_pred = tf.nn.softmax(tf.matmul(X, W) + bias)\n",
    "\n",
    "#代价函数\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(y_pred + 1e-10), reduction_indices=1)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#加入优化算法：随机梯度下降算法\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.构建训练迭代过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, total loss=2185.383065543\n",
      "Epoch: 0002, total loss=1399.061143801\n",
      "Epoch: 0003, total loss=1381.790588486\n",
      "Epoch: 0004, total loss=1367.578045154\n",
      "Epoch: 0005, total loss=1355.553322729\n",
      "Epoch: 0006, total loss=1344.684032992\n",
      "Epoch: 0007, total loss=1334.926935668\n",
      "Epoch: 0008, total loss=1325.748763263\n",
      "Epoch: 0009, total loss=1316.905705124\n",
      "Epoch: 0010, total loss=1308.393036272\n",
      "Epoch: 0011, total loss=1300.311208090\n",
      "Epoch: 0012, total loss=1292.830575945\n",
      "Epoch: 0013, total loss=1286.138685631\n",
      "Epoch: 0014, total loss=1280.230724424\n",
      "Epoch: 0015, total loss=1274.808053114\n",
      "Epoch: 0016, total loss=1269.679954715\n",
      "Epoch: 0017, total loss=1264.822800308\n",
      "Epoch: 0018, total loss=1260.253172231\n",
      "Epoch: 0019, total loss=1255.990858019\n",
      "Epoch: 0020, total loss=1252.050416565\n",
      "Epoch: 0021, total loss=1248.434137513\n",
      "Epoch: 0022, total loss=1245.128064301\n",
      "Epoch: 0023, total loss=1242.101214024\n",
      "Epoch: 0024, total loss=1239.303256493\n",
      "Epoch: 0025, total loss=1236.671945274\n",
      "Epoch: 0026, total loss=1234.142368945\n",
      "Epoch: 0027, total loss=1231.674658719\n",
      "Epoch: 0028, total loss=1229.260835342\n",
      "Epoch: 0029, total loss=1226.883116868\n",
      "Epoch: 0030, total loss=1224.829823363\n",
      "Epoch: 0031, total loss=1228.789664542\n",
      "Epoch: 0032, total loss=1221.467817727\n",
      "Epoch: 0033, total loss=1224.687780784\n",
      "Epoch: 0034, total loss=1218.321698880\n",
      "Epoch: 0035, total loss=1220.231459059\n",
      "Epoch: 0036, total loss=1215.598901818\n",
      "Epoch: 0037, total loss=1215.592869844\n",
      "Epoch: 0038, total loss=1212.180133171\n",
      "Epoch: 0039, total loss=1208.824826475\n",
      "Epoch: 0040, total loss=1058.694956316\n",
      "Epoch: 0041, total loss=1260.334138707\n",
      "Epoch: 0042, total loss=1253.075081506\n",
      "Epoch: 0043, total loss=1235.249579207\n",
      "Epoch: 0044, total loss=1231.961130536\n",
      "Epoch: 0045, total loss=1168.724770689\n",
      "Epoch: 0046, total loss=1198.396128839\n",
      "Epoch: 0047, total loss=1188.226916675\n",
      "Epoch: 0048, total loss=1180.065634215\n",
      "Epoch: 0049, total loss=1175.892046712\n",
      "Epoch: 0050, total loss=1165.313221701\n",
      "Epoch: 0051, total loss=1172.082891832\n",
      "Epoch: 0052, total loss=1172.444671550\n",
      "Epoch: 0053, total loss=1172.904619755\n",
      "Epoch: 0054, total loss=1173.033684453\n",
      "Epoch: 0055, total loss=1172.840371593\n",
      "Epoch: 0056, total loss=1172.424994723\n",
      "Epoch: 0057, total loss=1171.914333998\n",
      "Epoch: 0058, total loss=1171.405852445\n",
      "Epoch: 0059, total loss=1170.952631824\n",
      "Epoch: 0060, total loss=1170.570752554\n",
      "Epoch: 0061, total loss=1170.258559559\n",
      "Epoch: 0062, total loss=1170.006044443\n",
      "Epoch: 0063, total loss=1169.802539507\n",
      "Epoch: 0064, total loss=1169.636415792\n",
      "Epoch: 0065, total loss=1169.499481421\n",
      "Epoch: 0066, total loss=1169.383859843\n",
      "Epoch: 0067, total loss=1169.285457856\n",
      "Epoch: 0068, total loss=1169.200233684\n",
      "Epoch: 0069, total loss=1169.123976600\n",
      "Epoch: 0070, total loss=1169.053768306\n",
      "Epoch: 0071, total loss=1168.989290533\n",
      "Epoch: 0072, total loss=1168.928212670\n",
      "Epoch: 0073, total loss=1168.870010870\n",
      "Epoch: 0074, total loss=1168.812043721\n",
      "Epoch: 0075, total loss=1168.755284705\n",
      "Epoch: 0076, total loss=1168.698107239\n",
      "Epoch: 0077, total loss=1168.639964860\n",
      "Epoch: 0078, total loss=1168.580151845\n",
      "Epoch: 0079, total loss=1168.518383246\n",
      "Epoch: 0080, total loss=1168.454489132\n",
      "Epoch: 0081, total loss=1168.386510709\n",
      "Epoch: 0082, total loss=1168.316708874\n",
      "Epoch: 0083, total loss=1168.242402919\n",
      "Epoch: 0084, total loss=1168.163346816\n",
      "Epoch: 0085, total loss=1168.080292241\n",
      "Epoch: 0086, total loss=1167.992133912\n",
      "Epoch: 0087, total loss=1167.898981287\n",
      "Epoch: 0088, total loss=1167.799876680\n",
      "Epoch: 0089, total loss=1167.696841174\n",
      "Epoch: 0090, total loss=1167.586597777\n",
      "Epoch: 0091, total loss=1167.471643173\n",
      "Epoch: 0092, total loss=1167.349989317\n",
      "Epoch: 0093, total loss=1167.221502856\n",
      "Epoch: 0094, total loss=1167.086919077\n",
      "Epoch: 0095, total loss=1166.946593854\n",
      "Epoch: 0096, total loss=1166.799327658\n",
      "Epoch: 0097, total loss=1166.646021992\n",
      "Epoch: 0098, total loss=1166.485912166\n",
      "Epoch: 0099, total loss=1166.319431517\n",
      "Epoch: 0100, total loss=1166.146325702\n",
      "Training complete!\n",
      "Accuracy on validation set: 0.687150836\n"
     ]
    }
   ],
   "source": [
    "# 存档入口\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    #以下为训练迭代，迭代100轮\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0.\n",
    "        for i in range(len(X_train)):\n",
    "            feed = {X: [X_train[i]], y: [y_train[i]]}\n",
    "            #通过session.run接口触发执行\n",
    "            _, loss = sess.run([train_op, cost], feed_dict=feed)\n",
    "            total_loss += loss\n",
    "        print('Epoch: %04d, total loss=%.9f' % (epoch + 1, total_loss))\n",
    "    print('Training complete!')\n",
    "    \n",
    "    #评估准确率\n",
    "    pred = sess.run(y_pred, feed_dict={X: X_val})\n",
    "    correct = np.equal(np.argmax(pred, 1), np.argmax(y_val, 1))\n",
    "    accuracy = np.mean(correct.astype(np.float32))\n",
    "    print(\"Accuracy on validation set: %.9f\" % accuracy)\n",
    "    \n",
    "    save_path = saver.save(sess, \"save/model.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###预测测试数据###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#读入测试数据集并完成预处理, \n",
    "testdata = pd.read_csv('data/test.csv')\n",
    "testdata = testdata.fillna(0)\n",
    "\n",
    "testdata['Sex'] = testdata['Sex'].apply(lambda s: 1 if s== 'male' else 0)\n",
    "X_test = testdata[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']]\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #加载模型存档\n",
    "    saver.restore(sess2, save_path)\n",
    "    #正向传播计算\n",
    "    predictions = np.argmax(sess2.run(y_pred, feed_dict={X:X_test}), 1)\n",
    "\n",
    "    #构建提交结果的数据结构，并将结果存储为csv文件\n",
    "    submission = pd.DataFrame({\n",
    "        \"PassengerId\": testdata[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    submission.to_csv(\"data/titanic_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
